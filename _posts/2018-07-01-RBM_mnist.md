---
layout: post
title: "RBM I am dreaming"
---

I always wanted to get my hands dirty with image processing. I had to start from somewhere, but the standard plethora of tutorial on how to use convolutional neural networks to do image classification always sounded a bit too boring for me. So the other day I was talking with a friend who works on Restricted Boltzmann Machines and that sparked my curiosity. Turns out that they are pretty cool and can be used for example in <a href="">image reconstruction</a> or, more in general, as an alternative to PCA for unsupervised feature extraction. Another cool application is that once properly trained they can "dream" images, i.e., generate images similar to the ones in the training dataset.

There's a scikit implementation of RBMs (see <a href=""></a>), and if you google it you can find some other implementations both in python and C++. However here I wanted to understand how RBMs work and so I implemented one myself in python. The implementation is vectorized with `numpy` arrays and so runs in parallel on the CPU in a decent time.

So let's dream and get started with it!

# H2 Introduction

A restricted Boltzmann machine is an energy-based probabilistic model. One has two sets of random variables, the visible variables $v$ and the hidden variables $h$. From a graph point of view we can represent a RBM as

![_config.yml]({{ site.baseurl }}/images/rbm2.png)

from which it is clear where the term "restricted" comes from: the hidden variables are conditionally independent given the visible ones and the visible variables are conditionally independent given the hidden ones. In other words, given $n_h$ hidden variables and $n_v$ visible variables,

\begin{align}
P(h|v) = \prod_{j=1}^{n_h}P(h_j|v) \quad\text{and}\quad P(v|h) = \prod_{i=1}^{n_v}P(v_i|h).
\end{align}

The joint probability distribution between $v$ and $h$ is defined as

\begin{align}
P(v,h) = \frac{e^{-E(v,h)}}{\sum_{h,v}e^{-E(v,h)}} = \frac{e^{-E(v,h)}}{Z},
\end{align}

where in the last equality we have defined the partition function $Z$, and $E(v,h)$ is the energy associated with a configuration of $h$ and $v$. The energy can be written as

\begin{align}
E(v,h) = -c^Th - b^Tv - v^TWh,
\end{align}

where $c\in\mathbb{R}^{n_h}$ ($b\in\mathbb{R}^{n_v}$) is a vector representing the weights associated to the edges connecting the hidden (visible) units with the bias unit and $W\in\mathbb{R}^{n_v\times n_h}$ is the weights matrix representing the factors associated with the edges connecting hidden and visible units.



# H4 Maximizing the likelihood

When we train the RBM we want to maximize the probability $P(v^t)$ of a training data $v^t$ with respect to the parameters $\theta = \{b,c,W\}$. We can turn this into a minimization problem of the marginal log-likelihood

\begin{align}
\mathcal{L}^t(\theta) \quad=&\quad {\rm \log} \left[P(v^t)\right] \\
=&\quad {\rm\log}\left[\sum_hP(v^t,h)\right] \\
=&\quad {\rm\log}\left[\sum_h e^{-E(v^t,h)}\right] - {\rm\log}\left[\sum_{h,v}e^{-E(v,h)}\right].
\end{align}

Notice how the second term (coming from the partition function) now does not depend on the observation $v^t$. We can find the gradient and then use simple gradient descent to tune the parameters and maximize $P(v^t)$. However, because of the second term in the above equation (which is a sum over all possible configurations of $h$ and $v$), the log-likelihood is intractable. This is also clear from the gradient of $\mathcal{L}^t(\theta)$ which is given by

\begin{align}
\nabla_\theta\mathcal{L^t}(\theta) \quad=&\quad -\frac{\sum_h e^{-E(v^t,h)}\frac{\partial E(v^t,h)}{\partial\theta}}{\sum_h e^{-E(v^t,h)}} + \frac{\sum_{h,v} e^{-E(v,h)}\frac{\partial E(v,h)}{\partial\theta}}{\sum_{h,v} e^{-E(v,h)}} \\
=&\quad-\mathbb{E}_{P(h|v^t)}\left[\frac{\partial E(v^t,h)}{\partial\theta}\right] + \mathbb{E}_{P(v,h)}\left[\frac{\partial E(v,h)}{\partial\theta}\right].
\end{align}

# H4 Contrastive Divergence and conditional distributions

From the above formula we see that in order to evaluate the gradient of the marginal log-likelihood, we need the expectation of the derivative of the energy. While we can obtain the first expectation by sampling only on $h$, for the second we would need to sample on both %h% and $v$. While this is possible with Gibbs sampling, the problem is still computationally intractable as we would need to draw a large amount of samples for $v$ and $h$. We can avoid this by still making use of Gibbs sampling, but replacing the expectation value by a point estimate at $v=\tilde{v},h=\tilde{h}$. This procedure is also called <a href="">Contrastive Divergence</a>, which can be summarized as:

<ul>
<li>start with $v=v^t$,</li>
<li>sample $h$ from $P(h|v=v^t)$,</li>
<li>sample $\tilde{v}$ from $P(v=\tilde{v}|h)$,</li>
<li>finally sample $\tilde{h}$ from $P(h=\tilde{h}|v=\tilde{v})$.</li>
</ul>

For the above procedure we need the conditional probabilities. These are easy to find, and for example one has

\begin{align}
P(h|v) = \frac{P(v,h)}{P(v)} = \frac{e^{c^Th + v^TWh}}{\sum_h e^{c^Th + v^TWh}}.
\end{align}

Since $h_j={0,1}\ \forall j=1,...,n_h$, for a single hidden variable we have

\begin{align}
P(h_j = 1|v) = \frac{e^{c_j + \sum_iv_iW_{ij}}}{1 + e^{c_j + \sum_iv_iW_{ij}}} = \sigma\left(c_j + \sum_iv_iW_{ij}\right),
\end{align}

where $\sigma(x)=1 / (1 + e^{-x})$ is the sigmoid function. Since we assum the single hidden units to be conditionally independent from each other, we then have

\begin{align}
P(h|v) = \prod_{j=1}^{n_h} \sigma\left(c_j + \sum_iv_iW_{ij}\right),
\end{align}

and equivalently

\begin{align}
P(v|h) = \prod_{i=1}^{n_v} \sigma\left(b_i + \sum_jW_{ij}h_j\right).
\end{align}




# H2 Learning process, feature weights and the first dreaming attempt

So, I followed <a href="">this excellent blog post</a> to get started with this project and then went onto some more advance math stuff


fk dsfds, ,mfmds lfdsl f




