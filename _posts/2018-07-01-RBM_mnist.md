---
layout: post
title: "RBM I am dreaming"
---

I always wanted to get my hands dirty with image processing. I had to start from somewhere, but the standard plethora of tutorial on how to use convolutional neural networks to do image classification always sounded a bit too boring for me. So the other day I was talking with a friend who works on Restricted Boltzmann Machines and that sparked my curiosity. Turns out that they are pretty cool and can be used for example in image reconstruction (more in general, they can be used in different domains for unsupervised feature extraction). Another cool application is that once trained on a set of images, they can "dream" new images, i.e., generate images similar to the ones in the training dataset.

If you want to try it out, scikit has an implementation of RBMs (see <a href=""></a>), and if you google it you can find some other implementations both in <a href="https://github.com/echen/restricted-boltzmann-machines">python</a> and <a = href="https://github.com/jdeng/rbm-mnist">C++</a>. However here I wanted to understand how RBMs work and so I implemented one myself in python. The implementation is vectorized with `numpy` arrays and so runs in parallel on the CPU in a decent amount of time. For reference I used the excellent paper <a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">A Practical Guide to Training Restricted Boltzmann Machines</a> from G. Hinton.

So let's dream and get started with it!

# Introduction

A restricted Boltzmann machine is an energy-based probabilistic model. One has two sets of random variables, the visible variables $v$ and the hidden variables $h$. From a graph point of view we can represent a RBM as

![_config.yml]({{ site.baseurl }}/images/rbm2.png)

where the red layer represents the visible variables $v$ and the blue layer the hidden units $h$. From the above figure is clear where the term "restricted" comes from: the hidden variables are conditionally independent given the visible ones and the visible variables are conditionally independent given the hidden ones. In other words, given $n_h$ hidden variables and $n_v$ visible variables,

$$
P(h|v) = \prod_{j=1}^{n_h}P(h_j|v) \quad\text{and}\quad P(v|h) = \prod_{i=1}^{n_v}P(v_i|h).
$$

The joint probability distribution between $v$ and $h$ is defined as

$$
P(v,h) = \frac{e^{-E(v,h)}}{\sum_{h,v}e^{-E(v,h)}} = \frac{e^{-E(v,h)}}{Z},
$$

where in the last equality we have defined the partition function $Z$, and $E(v,h)$ is the energy associated with a configuration of $h$ and $v$. The energy can be written as

$$
E(v,h) = -c^Th - b^Tv - v^TWh,
$$

where $c\in\mathbb{R}^{n_h}$ ($b\in\mathbb{R}^{n_v}$) is a vector representing the weights associated to the edges connecting the hidden (visible) units with the bias unit and $W\in\mathbb{R}^{n_v\times n_h}$ is the weights matrix representing the factors associated with the edges connecting hidden and visible units.



### Maximizing the likelihood

When we train the RBM we want to maximize the probability $P(v^t)$ of a training data $v^t$ with respect to the parameters $\theta = \{b,c,W\}$. We can turn this into a minimization problem of the marginal log-likelihood

$$\begin{align}
\mathcal{L}^t(\theta) =& {\rm \log} \left[P(v^t)\right]={\rm\log}\left[\sum_hP(v^t,h)\right] \notag \\
=& {\rm\log}\left[\sum_h e^{-E(v^t,h)}\right] - {\rm\log}\left[\sum_{h,v}e^{-E(v,h)}\right].
\end{align}$$

Notice how the second term (coming from the partition function) now does not depend on the observation $v^t$. We can find the gradient and then use simple gradient descent to tune the parameters and maximize $P(v^t)$. However, because of the second term in the above equation (which is a sum over all possible configurations of $h$ and $v$), the log-likelihood is intractable. This is also clear from the gradient of $\mathcal{L}^t(\theta)$ which is given by

$$\begin{align}
\nabla_\theta\mathcal{L^t}(\theta) =& -\frac{\sum_h e^{-E(v^t,h)}\frac{\partial E(v^t,h)}{\partial\theta}}{\sum_h e^{-E(v^t,h)}} + \frac{\sum_{h,v} e^{-E(v,h)}\frac{\partial E(v,h)}{\partial\theta}}{\sum_{h,v} e^{-E(v,h)}} \notag \\
=& -\mathbb{E}_{P(h|v^t)}\left[\frac{\partial E(v^t,h)}{\partial\theta}\right] + \mathbb{E}_{P(v,h)}\left[\frac{\partial E(v,h)}{\partial\theta}\right].
\end{align}$$

### Contrastive Divergence and conditional distributions

From the above formula we see that in order to evaluate the gradient of the marginal log-likelihood, we need the expectation of the derivative of the energy. While we can obtain the first expectation by sampling only on $h$, for the second we would need to sample on both $h$ and $v$. While this is possible with Gibbs sampling, the problem is still computationally intractable as we would need to draw a large amount of samples for $v$ and $h$. We can avoid this by still making use of Gibbs sampling, but replacing the expectation value by a point estimate at $v=\tilde{v},h=\tilde{h}$. This procedure is also called <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.443.5593&rep=rep1&type=pdf">Contrastive Divergence</a>, which can be summarized as:

<ul>
<li>start with $v=v^t$,</li>
<li>sample $h$ from $P(h|v=v^t)$,</li>
<li>sample $\tilde{v}$ from $P(v=\tilde{v}|h)$,</li>
<li>finally sample $\tilde{h}$ from $P(h=\tilde{h}|v=\tilde{v})$.</li>
</ul>

For the above procedure we need the conditional probabilities. These are easy to find, and for example one has

$$
P(h|v) = \frac{P(v,h)}{P(v)} = \frac{e^{c^Th + v^TWh}}{\sum_h e^{c^Th + v^TWh}}.
$$

Since $h_j={0,1}\ \forall j=1,...,n_h$, for a single hidden variable we have

$$
P(h_j = 1|v) = \frac{e^{c_j + \sum_iv_iW_{ij}}}{1 + e^{c_j + \sum_iv_iW_{ij}}} = \sigma\left(c_j + \sum_iv_iW_{ij}\right),
$$

where $\sigma(x)=1 / (1 + e^{-x})$ is the sigmoid function. Since we assum the single hidden units to be conditionally independent from each other, we then have

$$
P(h|v) = \prod_{j=1}^{n_h} \sigma\left(c_j + \sum_iv_iW_{ij}\right),
$$

and equivalently

$$
P(v|h) = \prod_{i=1}^{n_v} \sigma\left(b_i + \sum_jW_{ij}h_j\right).
$$

### Derivatives of the energy

The last ingredient we need are the derivatives of the energy with respect to the model parameters $\theta=\{b,c,W\}$, which I summarise below:

$$\begin{align}
\frac{\partial E(v,h)}{\partial W}  = vh^T \quad,\quad\frac{\partial E(v,h)}{\partial b} = v \quad,\quad\frac{\partial E(v,h)}{\partial c}= h.
\end{align}$$

The gradients of the marginal log-likelihood are then calculated via

$$\begin{align}
\nabla_W\mathcal{L}(\theta) = \mathbb{E}_{P(h|v^t)}\left[v^th^T\right] - \mathbb{E}_{P(v,h)}\left[vh^T\right]= \langle h^T\rangle v^t - \langle vh^T\rangle
\end{align}$$

$$\begin{align}
\nabla_b\mathcal{L}(\theta) = \mathbb{E}_{P(h|v^t)}\left[v^t\right] - \mathbb{E}_{P(v,h)}\left[v\right]= v^t - \langle v\rangle
\end{align}$$

$$\begin{align}
\nabla_c\mathcal{L}(\theta) = \mathbb{E}_{P(h|v^t)}\left[h\right] - \mathbb{E}_{P(v,h)}\left[h\right]= \langle h^t\rangle - \langle h\rangle
\end{align}$$

The weights and biases can then be updated iteratively with gradient descent using the general formula

$$\begin{align}
\theta^{(new)} = \theta^{(old)} - \alpha\nabla_\theta\mathcal{L}(\theta^{(old)})\
\end{align}$$

where $\alpha$ is the learning rate.


# Small RBM, weights representation and reconstruction

So, I coded up all the stuff above (you can find the complete code and my custom RBM class <a href="">here</a>) and I gave it a try to see if I could train a RBM. Here I use the <a href="">MNIST</a> dataset to train the RBM on hand-written digits (from 0 to 9). I use all the 60000 images, which are 28x28 pixels each. I flatten the images and so I have 784 input units. Each $v^t$ will then be a 784-dimensional vector. I also convert all images to black and white. A sample of the images used for the training are shown in Figure 2.

{% include image.html url="/images/sample_mnist.png" description="<b>Figure 2</b>. A sample of training images from the MNIST data set." %}

To start simple, I first tried to train the RBM with 10 hidden units. I initialize the weights to random values and I train for 10 epochs. To monitor the training progress we can plot, at each epoch, the value of the weights. Each row in Figure 3 represents one training epoch and each column represents a certain hidden unit. Already after the first epoch the RBM has learned interesting features from the data set. On the other hand, we also see that some units train faster than others: units 4,5 and 10 appear to be much slower to train than the others!

{% include image.html url="/images/joined_nh10_train.jpeg" description="<b>Figure 3</b>. Image representation of the weights associated with each hidden unit (columns) for varying training epochs (rows)." %}

Although we see some learning happening in the above figure, it turns out that even if one trains the RBM for longer, the reconstruction is not very good. This is shown in Figure 4: the first column represents samples from the MNIST dataset; the second column is the reconstruction of the image from the trained RBM; the subsequent ten images in each row represent the hidden unit weights. On top of each hidden unit I also printed the probability that the unit is activated and I colored the border in red for units which have an appreciable probability of being activated. 

{% include image.html url="/images/recon_sample.png" description="<b>Figure 4</b>. Sample from the MNIST training data set (first column) together with the reconstruction from the trained RBM (second column) and the final weights of the hidden units. In red are shown the hidden units which are likely to be activated for the given input. On top of each hidden unit is the pobability that the unit will be activated." %}

I suppose one of the problems is that with only 10 hidden units the model is not that flexible. Sure, it can recognize the more general patterns and easy digits which are more or less uniform in shape across all the dataset (for example the digit 1), but it fails for more complex patterns (for example, the reconstruction is pretty bad for the digit 4).

# Large RBM: let's attempt to dream

Since it seems like 10 hidden units are not enough, I trained two more RBMs with 100 and 250 hidden units for 20 and 10 epochs, respectively (and decreasing the learning rate at each epoch). The results after training are shown in Figure 5 and Figure 6.

{% include image.html url="/images/recon_sample_nh100.png" description="<b>Figure 5</b>. Sample from the MNIST training data set (first column) together with the reconstruction from the trained RBM (second column). The last ten columns show the weights of the 10 most probable hidden units to be activated given the input. On top of each of these images. On top of each hidden unit is the pobability that the unit will be activated." %}

{% include image.html url="/images/recon_sample_nh250.png" description="<b>Figure 6</b>. Sample from the MNIST training data set (first row) together with the reconstruction from the trained RBM with 250 hidden units (second row)." %}

Finally, in Figure 7 and Figure 8, I show the image representations of the weights of each hidden unit for the two large RBMs with 100 and 250 hidden units, respectively.

{% include image.html url="/images/weights_100_1.png" description="<b>Figure 7</b>. Image representation of the weights of the hidden units for the RBM used to generate the results in Figure 5." %}

{% include image.html url="/images/weights_250_1.png" description="<b>Figure 8</b>. Image representation of the weights of the hidden units for the RBM used to generate the results in Figure 6." %}

# Sampling reconstructions

# Python code

### RBM class

